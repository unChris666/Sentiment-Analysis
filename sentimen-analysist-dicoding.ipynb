{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":["0oRZWS66Urjs"],"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12503921,"sourceType":"datasetVersion","datasetId":7891582},{"sourceId":12504023,"sourceType":"datasetVersion","datasetId":7891659}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preprocesing","metadata":{"id":"irwpfX-zh64V"}},{"cell_type":"markdown","source":"## Import Library","metadata":{"id":"XJGPoWhAgfIJ"}},{"cell_type":"code","source":"# Download NLTK data (tokenizer dan stopwords)\nimport nltk\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download('stopwords')\nimport pandas as pd\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport spacy\n# from google.colab import drive\n\n# Load spaCy model bahasa Inggris\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load stopword bahasa Inggris dari NLTK\nstop_words = set(stopwords.words('english'))","metadata":{"id":"HVyy6pimfcpW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e24dd7a0-d040-497b-cdfc-f8f17f648c7f","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:22:14.817366Z","iopub.execute_input":"2025-07-19T03:22:14.817562Z","iopub.status.idle":"2025-07-19T03:22:25.800720Z","shell.execute_reply.started":"2025-07-19T03:22:14.817544Z","shell.execute_reply":"2025-07-19T03:22:25.799912Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load Data","metadata":{"id":"njsXokoxgcHc"}},{"cell_type":"code","source":"# Mount Google Drive and load data\n# drive.mount('/content/drive')\n# file_path = '/content/drive/MyDrive/Dataset/ChatGPT_Reviews99k_Cleaned.csv'\nfile_path = '/kaggle/input/sentiment-analysist/chatGPT_clean_reviews.csv'\ndf = pd.read_csv(file_path)","metadata":{"id":"jyQIY0bIfjpA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2f4caac-13f5-4015-f482-3172f799dc61","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:22:25.801456Z","iopub.execute_input":"2025-07-19T03:22:25.802011Z","iopub.status.idle":"2025-07-19T03:22:26.794156Z","shell.execute_reply.started":"2025-07-19T03:22:25.801990Z","shell.execute_reply":"2025-07-19T03:22:26.793599Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Preprocessed","metadata":{"id":"xdqOtxuRgZ-s"}},{"cell_type":"code","source":"def preprocess_text(text):\n    # Lowercase\n    text = text.lower()\n\n    # Hapus angka\n    text = re.sub(r'\\d+', '', text)\n\n    # Hapus tanda baca\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Hapus spasi berlebih\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # Tokenisasi\n    tokens = word_tokenize(text)\n\n    # Hapus stopword\n    tokens = [word for word in tokens if word not in stop_words]\n\n    # Lemmatization\n    doc = nlp(' '.join(tokens))\n    lemmas = [token.lemma_ for token in doc]\n\n    return ' '.join(lemmas)\n","metadata":{"id":"vzvPM_CIgYrD","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:22:26.796111Z","iopub.execute_input":"2025-07-19T03:22:26.796723Z","iopub.status.idle":"2025-07-19T03:22:26.801231Z","shell.execute_reply.started":"2025-07-19T03:22:26.796702Z","shell.execute_reply":"2025-07-19T03:22:26.800488Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df['preprocessed'] = df['content'].astype(str).apply(preprocess_text)","metadata":{"id":"jHSnrkY2glah","colab":{"base_uri":"https://localhost:8080/","height":686},"outputId":"99fa439b-d2f2-4006-c68d-2ecd1a2935ae","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:22:26.802143Z","iopub.execute_input":"2025-07-19T03:22:26.802367Z","iopub.status.idle":"2025-07-19T03:32:02.498544Z","shell.execute_reply.started":"2025-07-19T03:22:26.802351Z","shell.execute_reply":"2025-07-19T03:32:02.498006Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Labeling","metadata":{"id":"2QRwmcnQgowe"}},{"cell_type":"code","source":"# Install & import VADER (sudah tersedia di NLTK)\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n\n# Inisialisasi analyzer\nvader = SentimentIntensityAnalyzer()\n","metadata":{"id":"rlosbByWgqnp","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:02.499271Z","iopub.execute_input":"2025-07-19T03:32:02.499519Z","iopub.status.idle":"2025-07-19T03:32:02.530964Z","shell.execute_reply.started":"2025-07-19T03:32:02.499492Z","shell.execute_reply":"2025-07-19T03:32:02.530254Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def get_vader_sentiment(text):\n    scores = vader.polarity_scores(text)\n    compound = scores['compound']\n\n    if compound >= 0.05:\n        return 'positive'\n    elif compound <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n\ndf['vader_sentiment'] = df['content'].apply(get_vader_sentiment)\n","metadata":{"id":"H17AO49zguN8","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:02.531726Z","iopub.execute_input":"2025-07-19T03:32:02.531942Z","iopub.status.idle":"2025-07-19T03:32:22.856337Z","shell.execute_reply.started":"2025-07-19T03:32:02.531926Z","shell.execute_reply":"2025-07-19T03:32:22.855541Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df['vader_sentiment'].value_counts()","metadata":{"id":"Hp7Vxtwlgvft","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:22.857129Z","iopub.execute_input":"2025-07-19T03:32:22.857327Z","iopub.status.idle":"2025-07-19T03:32:22.875727Z","shell.execute_reply.started":"2025-07-19T03:32:22.857311Z","shell.execute_reply":"2025-07-19T03:32:22.874982Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"vader_sentiment\npositive    79455\nnegative    12250\nneutral      7295\nName: count, dtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Export Data","metadata":{"id":"t1TsZyQ8g0Af"}},{"cell_type":"code","source":"df.to_csv('ChatGPT_Review99k_Preprocesed.csv', index=False)","metadata":{"id":"Lr3aMvTtgvcv","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:22.876543Z","iopub.execute_input":"2025-07-19T03:32:22.876849Z","iopub.status.idle":"2025-07-19T03:32:24.248784Z","shell.execute_reply.started":"2025-07-19T03:32:22.876828Z","shell.execute_reply":"2025-07-19T03:32:24.248142Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Scheme I - Bi-GRU + GloVe + class_weight","metadata":{"id":"0oRZWS66Urjs"}},{"cell_type":"markdown","source":"## Import Library","metadata":{"id":"N87nAXpXZc5a"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n# from imblearn.over_sampling import SMOTE\nfrom google.colab import drive\nfrom sklearn.utils.class_weight import compute_class_weight\nnp.random.seed(112)\ntf.random.set_seed(112)","metadata":{"id":"Cx7ujONWLBb_","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:24.251015Z","iopub.execute_input":"2025-07-19T03:32:24.251276Z","iopub.status.idle":"2025-07-19T03:32:38.834851Z","shell.execute_reply.started":"2025-07-19T03:32:24.251260Z","shell.execute_reply":"2025-07-19T03:32:38.834223Z"}},"outputs":[{"name":"stderr","text":"2025-07-19 03:32:26.230534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752895946.505153      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752895946.580570      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Prepare Data","metadata":{"id":"cxe4Z-xzNN9I"}},{"cell_type":"code","source":"# Mount Google Drive and load data\n# drive.mount('/content/drive')\n# file_path = '/content/drive/MyDrive/Dataset/ChatGPT_Review99k_Preprocessed.csv'\nfile_path = '/kaggle/working/ChatGPT_Review99k_Preprocesed.csv'\ndf = pd.read_csv(file_path)\n\n# Prepare data and labels\ndf = df[['preprocessed', 'vader_sentiment']].copy()\ndf.dropna(subset=['preprocessed'], inplace=True)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['vader_sentiment'])\ndf.info()","metadata":{"id":"2Qh6Rz_nNN9I","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:38.835572Z","iopub.execute_input":"2025-07-19T03:32:38.836143Z","iopub.status.idle":"2025-07-19T03:32:39.728898Z","shell.execute_reply.started":"2025-07-19T03:32:38.836121Z","shell.execute_reply":"2025-07-19T03:32:39.728181Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 99000 entries, 0 to 98999\nData columns (total 3 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   preprocessed       99000 non-null  object\n 1   vader_sentiment    99000 non-null  object\n 2   sentiment_encoded  99000 non-null  int64 \ndtypes: int64(1), object(2)\nmemory usage: 2.3+ MB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Tokenize Text Data","metadata":{"id":"co8RnR6c4QK3"}},{"cell_type":"code","source":"print(\"\\n Tokenizing text data...\")\n# Define parameters\nMAX_WORDS = 10000 # Maximum number of words to keep\nMAX_SEQUENCE_LENGTH = 150 # Maximum length of all sequences\n\n# Create and fit the tokenizer\ntokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<oov>\")\ntokenizer.fit_on_texts(df['preprocessed'].values)\nword_index = tokenizer.word_index\n\n# Convert text to padded sequences\nsequences = tokenizer.texts_to_sequences(df['preprocessed'].values)\npadded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')","metadata":{"id":"dmEqhG6qb4y_","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:39.729707Z","iopub.execute_input":"2025-07-19T03:32:39.730007Z","iopub.status.idle":"2025-07-19T03:32:42.053340Z","shell.execute_reply.started":"2025-07-19T03:32:39.729980Z","shell.execute_reply":"2025-07-19T03:32:42.052565Z"}},"outputs":[{"name":"stdout","text":"\n Tokenizing text data...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Load Pre-Trained GLOVE Embeddings","metadata":{"id":"53aK8ixb4a4j"}},{"cell_type":"code","source":"print(\"\\n Loading GloVe Embeddings...\")\n# IMPORTANT: Download GloVe embeddings first, e.g., 'glove.6B.100d.txt'\n# from https://nlp.stanford.edu/projects/glove/\n# GLOVE_PATH = '/content/drive/MyDrive/Dataset/glove.6B.100d.txt'\nGLOVE_PATH = '/kaggle/input/glove6b/glove.6B.100d.txt'\nEMBEDDING_DIM = 100 # Must match the GloVe file's dimension\n\nembeddings_index = {}\ntry:\n    with open(GLOVE_PATH, encoding=\"utf8\") as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    print(f\"Found {len(embeddings_index)} word vectors.\")\nexcept FileNotFoundError:\n    print(\"GloVe file not found. Please update GLOVE_PATH.\")\n    # Create an empty dictionary if the file is not found\n    embeddings_index = {}\n\n# Create the embedding matrix\nembedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i < MAX_WORDS:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n","metadata":{"id":"2xOTBNosZJQp","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:42.054272Z","iopub.execute_input":"2025-07-19T03:32:42.054562Z","iopub.status.idle":"2025-07-19T03:32:53.047678Z","shell.execute_reply.started":"2025-07-19T03:32:42.054537Z","shell.execute_reply":"2025-07-19T03:32:53.047007Z"}},"outputs":[{"name":"stdout","text":"\n Loading GloVe Embeddings...\nFound 400000 word vectors.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Splitting Data","metadata":{"id":"tA_SMhan4wkj"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = padded_sequences\ny = df['sentiment_encoded'].values\n\n# Split data (ensure stratification to maintain distribution)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Testing data shape: {X_test.shape}\")","metadata":{"id":"75I-jBrceAb0","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:53.048387Z","iopub.execute_input":"2025-07-19T03:32:53.048645Z","iopub.status.idle":"2025-07-19T03:32:53.128052Z","shell.execute_reply.started":"2025-07-19T03:32:53.048615Z","shell.execute_reply":"2025-07-19T03:32:53.127388Z"}},"outputs":[{"name":"stdout","text":"Training data shape: (79200, 150)\nTesting data shape: (19800, 150)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Class Weight","metadata":{"id":"s5qtYwscX8J7"}},{"cell_type":"code","source":"# Menghitung bobot agar kelas minoritas mendapat perhatian lebih\nclass_weights = compute_class_weight(\n    'balanced',\n    classes=np.unique(y_train),\n    y=y_train\n)\n# Mengubahnya menjadi format dictionary yang dibutuhkan Keras\nclass_weight_dict = dict(enumerate(class_weights))\n\nprint(\"Class Weights yang akan digunakan:\", class_weight_dict)","metadata":{"id":"6tybe8wjUNGl","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:53.128811Z","iopub.execute_input":"2025-07-19T03:32:53.129064Z","iopub.status.idle":"2025-07-19T03:32:53.227256Z","shell.execute_reply.started":"2025-07-19T03:32:53.129047Z","shell.execute_reply":"2025-07-19T03:32:53.226645Z"}},"outputs":[{"name":"stdout","text":"Class Weights yang akan digunakan: {0: 2.693877551020408, 1: 4.523646333104867, 2: 0.4153294317538229}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"model = Sequential([\n    Embedding(\n        input_dim=MAX_WORDS,\n        output_dim=EMBEDDING_DIM,\n        weights=[embedding_matrix],\n        input_length=MAX_SEQUENCE_LENGTH,\n        trainable=True, # Tetap fine-tuning untuk performa terbaik\n        mask_zero=True\n    ),\n    Bidirectional(GRU(128, return_sequences=True)),\n    Bidirectional(GRU(64, return_sequences=False)),\n    Dropout(0.5), # Menambahkan dropout untuk mencegah overfitting\n    Dense(3, activation='softmax') # Langsung ke output layer\n])\n\n# Compile model seperti biasa\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()","metadata":{"id":"robH1a80Uqg_","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:53.227926Z","iopub.execute_input":"2025-07-19T03:32:53.228162Z","iopub.status.idle":"2025-07-19T03:32:54.938400Z","shell.execute_reply.started":"2025-07-19T03:32:53.228141Z","shell.execute_reply":"2025-07-19T03:32:54.937826Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\nI0000 00:00:1752895973.574786      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752895973.575523      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m1,000,000\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Training","metadata":{"id":"ivlI1TXCYEiL"}},{"cell_type":"code","source":"history = model.fit(\n    X_train,\n    y_train,\n    epochs=10,\n    batch_size=64,\n    validation_data=(X_test, y_test),\n    class_weight=class_weight_dict, # <-- INI KUNCINYA\n    verbose=2\n)\n\nfinal_train_accuracy = history.history['accuracy'][-1]\nfinal_val_accuracy = history.history['val_accuracy'][-1]\nfinal_train_loss = history.history['loss'][-1]\nfinal_val_loss = history.history['val_loss'][-1]\n\n# Tampilkan hasilnya dengan format yang mudah dibaca\nprint(f\"Akurasi Pelatihan Final: {final_train_accuracy*100:.2f}%\")\nprint(f\"Loss Pelatihan Final: {final_train_loss:.4f}\")\nprint(f\"Akurasi Validasi Final: {final_val_accuracy*100:.2f}%\")\nprint(f\"Loss Validasi Final: {final_val_loss:.4f}\")","metadata":{"id":"f-ZvtReeUtWC","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:32:54.939149Z","iopub.execute_input":"2025-07-19T03:32:54.939965Z","iopub.status.idle":"2025-07-19T03:36:18.289501Z","shell.execute_reply.started":"2025-07-19T03:32:54.939940Z","shell.execute_reply":"2025-07-19T03:36:18.288805Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752895981.795657      98 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"1238/1238 - 28s - 23ms/step - accuracy: 0.8331 - loss: 0.5275 - val_accuracy: 0.8845 - val_loss: 0.3420\nEpoch 2/10\n1238/1238 - 20s - 16ms/step - accuracy: 0.8892 - loss: 0.3724 - val_accuracy: 0.8957 - val_loss: 0.3040\nEpoch 3/10\n1238/1238 - 20s - 16ms/step - accuracy: 0.9044 - loss: 0.3188 - val_accuracy: 0.9085 - val_loss: 0.2738\nEpoch 4/10\n1238/1238 - 20s - 16ms/step - accuracy: 0.9156 - loss: 0.2683 - val_accuracy: 0.9081 - val_loss: 0.2841\nEpoch 5/10\n1238/1238 - 19s - 16ms/step - accuracy: 0.9266 - loss: 0.2188 - val_accuracy: 0.9119 - val_loss: 0.2824\nEpoch 6/10\n1238/1238 - 19s - 16ms/step - accuracy: 0.9379 - loss: 0.1708 - val_accuracy: 0.9130 - val_loss: 0.3054\nEpoch 7/10\n1238/1238 - 19s - 15ms/step - accuracy: 0.9455 - loss: 0.1392 - val_accuracy: 0.9172 - val_loss: 0.3204\nEpoch 8/10\n1238/1238 - 19s - 16ms/step - accuracy: 0.9539 - loss: 0.1152 - val_accuracy: 0.9169 - val_loss: 0.3372\nEpoch 9/10\n1238/1238 - 19s - 16ms/step - accuracy: 0.9625 - loss: 0.0939 - val_accuracy: 0.9173 - val_loss: 0.3666\nEpoch 10/10\n1238/1238 - 19s - 15ms/step - accuracy: 0.9678 - loss: 0.0753 - val_accuracy: 0.9106 - val_loss: 0.3985\nAkurasi Pelatihan Final: 96.78%\nLoss Pelatihan Final: 0.0753\nAkurasi Validasi Final: 91.06%\nLoss Validasi Final: 0.3985\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Testing","metadata":{"id":"XzPuCzPuYGuE"}},{"cell_type":"code","source":"print(\"\\n 4. Mengevaluasi model...\")\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\nFinal Test Accuracy: {accuracy*100:.2f}%\")","metadata":{"id":"GzBF_iXmVBtb","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:18.290313Z","iopub.execute_input":"2025-07-19T03:36:18.290546Z","iopub.status.idle":"2025-07-19T03:36:21.762304Z","shell.execute_reply.started":"2025-07-19T03:36:18.290529Z","shell.execute_reply":"2025-07-19T03:36:21.761657Z"}},"outputs":[{"name":"stdout","text":"\n 4. Mengevaluasi model...\n\nFinal Test Accuracy: 91.06%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"Q4FUetnbYIiB"}},{"cell_type":"code","source":"new_reviews = [\n    \"This app is absolutely fantastic! It works perfectly and has all the features I need.\",\n    \"I'm really disappointed with the latest update. It's buggy and crashes all the time.\",\n    \"The application is okay, it does what it says but nothing more.\",\n    \"I hate this, it's the worst experience I've ever had.\",\n    \"Wow, I love it! So useful and easy to use.\",\n    \"just it\"\n]\n\n# 1. Pra-pemrosesan data baru (HARUS SAMA DENGAN DATA LATIH)\n#    a. Tokenisasi -> mengubah teks menjadi sekuens integer\nprocessed_sequences = tokenizer.texts_to_sequences(new_reviews)\n\n#    b. Padding -> menyeragamkan panjang sekuens\nprocessed_padded = tf.keras.preprocessing.sequence.pad_sequences(\n    processed_sequences,\n    maxlen=MAX_SEQUENCE_LENGTH, # Gunakan max_length yang sama\n    padding='post',\n    truncating='post'\n)\n\n# 2. Lakukan Prediksi\nprint(\"\\n Melakukan prediksi pada ulasan baru...\")\npredictions = model.predict(processed_padded)\n\n# 3. Ubah hasil prediksi (probabilitas) menjadi label kelas\n#    a. Ambil indeks kelas dengan probabilitas tertinggi\npredicted_class_indices = np.argmax(predictions, axis=1)\n\n#    b. Gunakan label_encoder untuk mengubah indeks kembali ke label teks\npredicted_class_labels = label_encoder.inverse_transform(predicted_class_indices)\n\n\n# 4. Tampilkan Hasil\nprint(\"\\n--- Hasil Inferensi ---\")\nfor i, review in enumerate(new_reviews):\n    print(f\"Ulasan: \\\"{review}\\\"\")\n    print(f\"Prediksi Sentimen: {predicted_class_labels[i].capitalize()}\")\n    print(\"-\" * 20)\n","metadata":{"id":"j0XtTxBcVw-c","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:21.763094Z","iopub.execute_input":"2025-07-19T03:36:21.763353Z","iopub.status.idle":"2025-07-19T03:36:22.326149Z","shell.execute_reply.started":"2025-07-19T03:36:21.763336Z","shell.execute_reply":"2025-07-19T03:36:22.325449Z"}},"outputs":[{"name":"stdout","text":"\n Melakukan prediksi pada ulasan baru...\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507ms/step\n\n--- Hasil Inferensi ---\nUlasan: \"This app is absolutely fantastic! It works perfectly and has all the features I need.\"\nPrediksi Sentimen: Positive\n--------------------\nUlasan: \"I'm really disappointed with the latest update. It's buggy and crashes all the time.\"\nPrediksi Sentimen: Negative\n--------------------\nUlasan: \"The application is okay, it does what it says but nothing more.\"\nPrediksi Sentimen: Negative\n--------------------\nUlasan: \"I hate this, it's the worst experience I've ever had.\"\nPrediksi Sentimen: Negative\n--------------------\nUlasan: \"Wow, I love it! So useful and easy to use.\"\nPrediksi Sentimen: Positive\n--------------------\nUlasan: \"just it\"\nPrediksi Sentimen: Neutral\n--------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Scheme II - BERT Fine-Tuning + class_weight","metadata":{"id":"ynoy6cRFU0Ua"}},{"cell_type":"markdown","source":"## Import Library","metadata":{"id":"Oaqbz3U5CSUz"}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import BertTokenizerFast\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"id":"SLDIR617J-Jk","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:22.327011Z","iopub.execute_input":"2025-07-19T03:36:22.327261Z","iopub.status.idle":"2025-07-19T03:36:33.021758Z","shell.execute_reply.started":"2025-07-19T03:36:22.327230Z","shell.execute_reply":"2025-07-19T03:36:33.021163Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Cek CUDA (GPU Nvidia)\nif torch.cuda.is_available():\n    print(f\" Available GPU: {torch.cuda.get_device_name(0)}\")\n    device = torch.device(\"cuda\")\nelse:\n    print(\" GPU not Avaiable\")\n    device = torch.device(\"cpu\")","metadata":{"id":"IPdBpKGh8jwq","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:33.022431Z","iopub.execute_input":"2025-07-19T03:36:33.022925Z","iopub.status.idle":"2025-07-19T03:36:33.027734Z","shell.execute_reply.started":"2025-07-19T03:36:33.022901Z","shell.execute_reply":"2025-07-19T03:36:33.026886Z"}},"outputs":[{"name":"stdout","text":" Available GPU: Tesla T4\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Prepare Data","metadata":{"id":"tfvmpF7INItx"}},{"cell_type":"code","source":"# Mount Google Drive and load data\n# from google.colab import drive\n# drive.mount('/content/drive')\n# file_path = '/content/drive/MyDrive/Dataset/ChatGPT_Review99k_Preprocessed.csv'\nfile_path = '/kaggle/working/ChatGPT_Review99k_Preprocesed.csv'\ndf = pd.read_csv(file_path)\n\n# Prepare data and labels\ndf = df[['preprocessed', 'vader_sentiment']].copy()\ndf.dropna(subset=['preprocessed'], inplace=True)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['vader_sentiment'])\ndf.info()","metadata":{"id":"2C2W0UpMNItx","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:33.028606Z","iopub.execute_input":"2025-07-19T03:36:33.029043Z","iopub.status.idle":"2025-07-19T03:36:33.872275Z","shell.execute_reply.started":"2025-07-19T03:36:33.029017Z","shell.execute_reply":"2025-07-19T03:36:33.871466Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 99000 entries, 0 to 98999\nData columns (total 3 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   preprocessed       99000 non-null  object\n 1   vader_sentiment    99000 non-null  object\n 2   sentiment_encoded  99000 non-null  int64 \ndtypes: int64(1), object(2)\nmemory usage: 2.3+ MB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Define column names and create label maps","metadata":{"id":"dL5XaOvwLohs"}},{"cell_type":"code","source":"TEXT_COLUMN = 'preprocessed'\nLABEL_COLUMN = 'sentiment_encoded'\nORIGINAL_LABEL_COLUMN = 'vader_sentiment'\n\n# Create mappings from existing encoded data\nunique_labels_df = df[[LABEL_COLUMN, ORIGINAL_LABEL_COLUMN]].drop_duplicates().sort_values(LABEL_COLUMN)\nid2label = dict(zip(unique_labels_df[LABEL_COLUMN], unique_labels_df[ORIGINAL_LABEL_COLUMN]))\nlabel2id = dict(zip(unique_labels_df[ORIGINAL_LABEL_COLUMN], unique_labels_df[LABEL_COLUMN]))\nnum_labels = len(id2label)\n\nprint(f\"ID to Label Map: {id2label}\")\nprint(f\"Number of Labels: {num_labels}\")","metadata":{"id":"gtNokAF-VAyV","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:33.873307Z","iopub.execute_input":"2025-07-19T03:36:33.873578Z","iopub.status.idle":"2025-07-19T03:36:33.889851Z","shell.execute_reply.started":"2025-07-19T03:36:33.873554Z","shell.execute_reply":"2025-07-19T03:36:33.889131Z"}},"outputs":[{"name":"stdout","text":"ID to Label Map: {0: 'negative', 1: 'neutral', 2: 'positive'}\nNumber of Labels: 3\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Split Data Train, Validation, and Test","metadata":{"id":"5jGbN-8NLtWV"}},{"cell_type":"code","source":"# training+validation (80%) dan test (20%)\ntrain_val_df, test_df = train_test_split(\n    df,\n    test_size=0.2, # 20% untuk data test\n    random_state=112,\n    stratify=df[LABEL_COLUMN]\n)\n\n# training (80% dari sisa) dan validation (20% dari sisa)\ntrain_df, val_df = train_test_split(\n    train_val_df,\n    test_size=0.2, # 20% dari 80% data awal untuk validasi\n    random_state=112,\n    stratify=train_val_df[LABEL_COLUMN]\n)\n\nprint(f\"Ukuran Data Training: {len(train_df)}\")\nprint(f\"Ukuran Data Validasi: {len(val_df)}\")\nprint(f\"Ukuran Data Test: {len(test_df)}\")","metadata":{"id":"bCYzpqL2Ia5r","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:33.890575Z","iopub.execute_input":"2025-07-19T03:36:33.890856Z","iopub.status.idle":"2025-07-19T03:36:33.952950Z","shell.execute_reply.started":"2025-07-19T03:36:33.890834Z","shell.execute_reply":"2025-07-19T03:36:33.952207Z"}},"outputs":[{"name":"stdout","text":"Ukuran Data Training: 63360\nUkuran Data Validasi: 15840\nUkuran Data Test: 19800\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Calculate Class Weights for Imbalance","metadata":{"id":"VOxcUcDxL1w6"}},{"cell_type":"code","source":"class_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_df[LABEL_COLUMN]),\n    y=train_df[LABEL_COLUMN].to_numpy()\n)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n\nprint(f\"\\nCalculated Class Weights: {class_weights}\")","metadata":{"id":"mY206vrtIj4F","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:33.953644Z","iopub.execute_input":"2025-07-19T03:36:33.953856Z","iopub.status.idle":"2025-07-19T03:36:33.965281Z","shell.execute_reply.started":"2025-07-19T03:36:33.953840Z","shell.execute_reply":"2025-07-19T03:36:33.964635Z"}},"outputs":[{"name":"stdout","text":"\nCalculated Class Weights: [2.69387755 4.52345256 0.41533107]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Tokenization and Dataset Creation","metadata":{"id":"d6GkySgZJoKR"}},{"cell_type":"code","source":"# --- Load Tokenizer ---\nMODEL_NAME = 'bert-base-uncased'\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n\n# --- Tokenize text and create Datasets ---\ntrain_encodings = tokenizer(train_df[TEXT_COLUMN].tolist(), truncation=True, padding=True, max_length=128)\nval_encodings = tokenizer(val_df[TEXT_COLUMN].tolist(), truncation=True, padding=True, max_length=128)\n\nclass SentimentDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = SentimentDataset(train_encodings, train_df[LABEL_COLUMN].tolist())\nval_dataset = SentimentDataset(val_encodings, val_df[LABEL_COLUMN].tolist())","metadata":{"id":"sU7r-Ib4VNhN","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:33.965942Z","iopub.execute_input":"2025-07-19T03:36:33.966134Z","iopub.status.idle":"2025-07-19T03:36:40.658090Z","shell.execute_reply.started":"2025-07-19T03:36:33.966120Z","shell.execute_reply":"2025-07-19T03:36:40.657503Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689a27c66b334114b89bc7d1065ef0f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ddbf0edf6248f5abf9f2b691c4aeb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb435a4abeb438aa2a62b2caac0f24b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664b584531844d1b81a446b7ee3df863"}},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## Training","metadata":{"id":"YkQ7ptUuKc4g"}},{"cell_type":"code","source":"# --- Load Pre-trained Model ---\nmodel = BertForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id\n)\n\n# --- Define Metrics ---\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# --- Custom Trainer for Weighted Loss ---\nclass WeightedLossTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        outputs = model(**inputs)\n        logits = outputs.get('logits')\n        labels = inputs.get('labels')\n\n        # Ambil device dari input tensor 'labels', bukan dari 'model.device'.\n        # bekerja di single-GPU (Colab) maupun multi-GPU (Kaggle).\n        device = labels.device\n        \n        # Pindahkan class weights ke device yang benar\n        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n\n        return (loss, outputs) if return_outputs else loss\n\n# --- Define Training Arguments (Dioptimasi dengan FP16) ---\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    logging_steps=100,\n    report_to=\"none\",\n    fp16=True,\n)\n\n# --- Instantiate and Run Trainer ---\ntrainer = WeightedLossTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    # tokenizer=tokenizer,\n)\n\n# Start fine-tuning\ntrainer.train()","metadata":{"id":"LN3sXBqjVZ5J","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T03:36:40.658975Z","iopub.execute_input":"2025-07-19T03:36:40.659206Z","iopub.status.idle":"2025-07-19T04:08:39.531948Z","shell.execute_reply.started":"2025-07-19T03:36:40.659188Z","shell.execute_reply":"2025-07-19T04:08:39.531193Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d1ce0980b4a401e9ceee72d957f2e87"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3960' max='3960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3960/3960 31:53, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.427900</td>\n      <td>0.394368</td>\n      <td>0.906313</td>\n      <td>0.910963</td>\n      <td>0.920632</td>\n      <td>0.906313</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.307500</td>\n      <td>0.425100</td>\n      <td>0.923737</td>\n      <td>0.925993</td>\n      <td>0.930707</td>\n      <td>0.923737</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3960, training_loss=0.4055168310801188, metrics={'train_runtime': 1915.5935, 'train_samples_per_second': 66.152, 'train_steps_per_second': 2.067, 'total_flos': 8335433073623040.0, 'train_loss': 0.4055168310801188, 'epoch': 2.0})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Jalankan evaluasi secara eksplisit pada validation set\nfinal_metrics = trainer.evaluate()\n\n# Cetak hasilnya\nprint(\"\\n--- Metrik Evaluasi Final ---\")\nprint(final_metrics)\n\naccuracy = final_metrics['eval_accuracy']\n\n\nprint(f\"\\n Akurasi Final: {accuracy:.4f}\")\n","metadata":{"id":"Ql8ssEmHaXG5","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:08:39.536149Z","iopub.execute_input":"2025-07-19T04:08:39.536738Z","iopub.status.idle":"2025-07-19T04:09:45.495284Z","shell.execute_reply.started":"2025-07-19T04:08:39.536719Z","shell.execute_reply":"2025-07-19T04:09:45.494628Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- Metrik Evaluasi Final ---\n{'eval_loss': 0.4251001179218292, 'eval_accuracy': 0.9237373737373737, 'eval_f1': 0.9259931836137475, 'eval_precision': 0.9307069876573975, 'eval_recall': 0.9237373737373737, 'eval_runtime': 65.952, 'eval_samples_per_second': 240.175, 'eval_steps_per_second': 1.88, 'epoch': 2.0}\n\n Akurasi Final: 0.9237\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## Testing","metadata":{"id":"Vwof2jr7Kf0e"}},{"cell_type":"code","source":"# Tokenisasi data test\ntest_encodings = tokenizer(test_df[TEXT_COLUMN].tolist(), truncation=True, padding=True, max_length=128)\ntest_dataset = SentimentDataset(test_encodings, test_df[LABEL_COLUMN].tolist())\n\n# --- Prediksi pada Data Test ---\ntest_predictions = trainer.predict(test_dataset)\n\n# metrikMdari hasil prediksi\ntest_metrics = test_predictions.metrics\nprint(\"\\n--- Testing Final Result---\")\nfor key, value in test_metrics.items():\n    print(f\"{key}: {value:.4f}\")","metadata":{"id":"ExfuPJMAJRa_","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:09:45.495913Z","iopub.execute_input":"2025-07-19T04:09:45.496177Z","iopub.status.idle":"2025-07-19T04:11:10.191564Z","shell.execute_reply.started":"2025-07-19T04:09:45.496149Z","shell.execute_reply":"2025-07-19T04:11:10.190838Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n--- Testing Final Result---\ntest_loss: 0.4137\ntest_accuracy: 0.9228\ntest_f1: 0.9252\ntest_precision: 0.9301\ntest_recall: 0.9228\ntest_runtime: 82.8030\ntest_samples_per_second: 239.1220\ntest_steps_per_second: 1.8720\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"gxzyeS4_3y-a"}},{"cell_type":"code","source":"def predict_sentiment(text):\n    \"\"\"\n    Fungsi untuk memprediksi sentimen dari sebuah teks tunggal.\n    \"\"\"\n    # Masukkan model ke mode evaluasi\n    trainer.model.eval()\n\n    # Tokenisasi input teks dan ubah menjadi PyTorch tensor\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n\n    # Pindahkan semua tensor ke device yang sesuai (GPU/CPU)\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n\n    # Lakukan prediksi tanpa menghitung gradient untuk efisiensi\n    with torch.no_grad():\n        outputs = trainer.model(**inputs)\n\n    # Ambil logits (skor mentah) dari output model\n    logits = outputs.logits\n\n    # Dapatkan ID kelas dengan skor tertinggi\n    predicted_class_id = torch.argmax(logits, dim=1).item()\n\n    # Kembalikan label kategorikal (contoh: 'positif') menggunakan map id2label\n    return id2label[predicted_class_id]","metadata":{"id":"78hNBgzV30aF","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:11:10.192433Z","iopub.execute_input":"2025-07-19T04:11:10.193150Z","iopub.status.idle":"2025-07-19T04:11:10.197500Z","shell.execute_reply.started":"2025-07-19T04:11:10.193131Z","shell.execute_reply":"2025-07-19T04:11:10.196970Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# List review baru yang akan diprediksi\nnew_reviews = [\n    \"This app is absolutely fantastic! It works perfectly and has all the features I need.\",\n    \"I'm really disappointed with the latest update. It's buggy and crashes all the time.\",\n    \"The application is okay, it does what it says but nothing more.\",\n    \"I hate this, it's the worst experience I've ever had.\",\n    \"Wow, I love it! So useful and easy to use.\",\n    \"Just it\"\n]\n\n# Loop melalui setiap review di dalam list\nfor review in new_reviews:\n    # Panggil fungsi prediksi yang sudah kita buat sebelumnya\n    predicted_label = predict_sentiment(review)\n\n    # Cetak hasilnya\n    print(f\"Review: \\\"{review}\\\"\")\n    print(f\"Prediksi: {predicted_label}\\n\")","metadata":{"id":"kMQikax77mgg","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:11:10.198215Z","iopub.execute_input":"2025-07-19T04:11:10.198535Z","iopub.status.idle":"2025-07-19T04:11:10.298914Z","shell.execute_reply.started":"2025-07-19T04:11:10.198509Z","shell.execute_reply":"2025-07-19T04:11:10.298306Z"}},"outputs":[{"name":"stdout","text":"Review: \"This app is absolutely fantastic! It works perfectly and has all the features I need.\"\nPrediksi: positive\n\nReview: \"I'm really disappointed with the latest update. It's buggy and crashes all the time.\"\nPrediksi: negative\n\nReview: \"The application is okay, it does what it says but nothing more.\"\nPrediksi: positive\n\nReview: \"I hate this, it's the worst experience I've ever had.\"\nPrediksi: negative\n\nReview: \"Wow, I love it! So useful and easy to use.\"\nPrediksi: positive\n\nReview: \"Just it\"\nPrediksi: neutral\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Scheme III - RoBERTa + class_weight","metadata":{"id":"53HroXweMLb6"}},{"cell_type":"code","source":"# !pip install transformers[torch] datasets pandas scikit-learn imbalanced-learn -q","metadata":{"id":"-U3VOE_yMMy_","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:11:10.299633Z","iopub.execute_input":"2025-07-19T04:11:10.299840Z","iopub.status.idle":"2025-07-19T04:11:10.303228Z","shell.execute_reply.started":"2025-07-19T04:11:10.299824Z","shell.execute_reply":"2025-07-19T04:11:10.302520Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight # Import for class weights\nfrom google.colab import drive # Import for Google Drive\n\n# Import the model class designed for fine-tuning\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nfrom torch.optim import AdamW # Correctly import AdamW from PyTorch","metadata":{"id":"vsQRYFdMMWDU","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:11:10.303877Z","iopub.execute_input":"2025-07-19T04:11:10.304064Z","iopub.status.idle":"2025-07-19T04:11:10.317435Z","shell.execute_reply.started":"2025-07-19T04:11:10.304049Z","shell.execute_reply":"2025-07-19T04:11:10.316727Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Configuration and Data Preparation","metadata":{"id":"RT0u5_06DLeN"}},{"cell_type":"code","source":"class Config:\n    MODEL_NAME = 'roberta-base'\n    MAX_LENGTH = 128  # Max sequence length for RoBERTa\n    # Reduce batch size for full fine-tuning to avoid memory issues on T4 GPU\n    BATCH_SIZE = 16\n    EPOCHS = 2        # 2 epochs is standard for full fine-tuning\n    LR = 2e-5         # A standard learning rate for fine-tuning\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nconfig = Config()\nprint(f\"Using device: {config.DEVICE}\")\n\n# --- Mount Google Drive and Load Your Data ---\n# drive.mount('/content/drive')\n# file_path = '/content/drive/MyDrive/Dataset/ChatGPT_Review99k_Preprocessed.csv'\nfile_path = '/kaggle/working/ChatGPT_Review99k_Preprocesed.csv'\ndf = pd.read_csv(file_path)\n\n# --- Prepare data columns ---\n# Select the necessary columns and handle any potential missing values\ndf = df[['preprocessed', 'vader_sentiment']].copy()\ndf.dropna(subset=['preprocessed'], inplace=True)\n\n# Rename columns to match what the rest of the script expects ('text' and 'label')\ndf.rename(columns={'preprocessed': 'text'}, inplace=True)\n\nprint(\"Initial Data Distribution:\")\nprint(df['vader_sentiment'].value_counts())\nprint(\"-\" * 30)\n\n# --- Map Labels to Integers ---\n# This map should correspond to the unique values in your 'vader_sentiment' column\nlabel_map = {'positive': 0, 'negative': 1, 'neutral': 2}\ndf['label'] = df['vader_sentiment'].map(label_map)\n\n# --- Split Data into Train, Validation, and Test sets ---\n# training+validation set and a test set.\ndf_train_val, df_test = train_test_split(\n    df,\n    test_size=0.2,  # 20% for the final test set\n    random_state=112,\n    stratify=df['label']\n)\n\n# Training and validation .\ndf_train, df_val = train_test_split(\n    df_train_val,\n    test_size=0.2, # This makes the validation set 10% of the original total\n    random_state=112,\n    stratify=df_train_val['label']\n)\n\n\nprint(f\"Training set size: {len(df_train)}\")\nprint(f\"Validation set size: {len(df_val)}\")\nprint(f\"Test set size: {len(df_test)}\")\nprint(\"-\" * 30)","metadata":{"id":"EtTs93QqMZAK","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:11:10.318203Z","iopub.execute_input":"2025-07-19T04:11:10.318442Z","iopub.status.idle":"2025-07-19T04:11:11.204936Z","shell.execute_reply.started":"2025-07-19T04:11:10.318421Z","shell.execute_reply":"2025-07-19T04:11:11.204341Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nInitial Data Distribution:\nvader_sentiment\npositive    79455\nnegative    12250\nneutral      7295\nName: count, dtype: int64\n------------------------------\nTraining set size: 63360\nValidation set size: 15840\nTest set size: 19800\n------------------------------\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## PyTorch Dataset for Fine-Tuning","metadata":{"id":"yQU7Gc0oD1kp"}},{"cell_type":"code","source":"class SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# --- Initialize Tokenizer ---\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n\n# --- Create Datasets ---\ntrain_dataset = SentimentDataset(\n    texts=df_train.text.to_numpy(),\n    labels=df_train.label.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=config.MAX_LENGTH\n)\n\nval_dataset = SentimentDataset(\n    texts=df_val.text.to_numpy(),\n    labels=df_val.label.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=config.MAX_LENGTH\n)\n\ntest_dataset = SentimentDataset(\n    texts=df_test.text.to_numpy(),\n    labels=df_test.label.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=config.MAX_LENGTH\n)","metadata":{"id":"gNChIivJNrVY","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:11:11.205657Z","iopub.execute_input":"2025-07-19T04:11:11.205873Z","iopub.status.idle":"2025-07-19T04:11:12.354551Z","shell.execute_reply.started":"2025-07-19T04:11:11.205848Z","shell.execute_reply":"2025-07-19T04:11:12.353986Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"720ba25c82c34a3da417a16eab85e61a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7f4f5e7c52463e8008bd5abc3cc03c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0828c0b01de949c59a95fa3a6e541024"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cedc9362132422cb3de5bdb826abed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8d808ce34f04c92b8456413abc7fcfe"}},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"## Handle Class Imbalance with Weighted Loss","metadata":{"id":"d3EinBkID9E4"}},{"cell_type":"code","source":"print(\"Calculating class weights for handling imbalance...\")\nclass_weights = compute_class_weight(\n    'balanced',\n    classes=np.unique(df_train.label),\n    y=df_train.label.to_numpy()\n)\nweights_tensor = torch.tensor(class_weights, dtype=torch.float).to(config.DEVICE)\nprint(f\"Calculated Class Weights: {weights_tensor.cpu().numpy()}\")\nprint(\"-\" * 30)","metadata":{"id":"eotAlivdNu6Z","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:11:12.355226Z","iopub.execute_input":"2025-07-19T04:11:12.355468Z","iopub.status.idle":"2025-07-19T04:11:12.370148Z","shell.execute_reply.started":"2025-07-19T04:11:12.355434Z","shell.execute_reply":"2025-07-19T04:11:12.369426Z"}},"outputs":[{"name":"stdout","text":"Calculating class weights for handling imbalance...\nCalculated Class Weights: [0.41533107 2.6938775  4.5234528 ]\n------------------------------\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## Define and Train the Full Model","metadata":{"id":"Y3bgeYe_EDB0"}},{"cell_type":"code","source":"# --- Load Model for Sequence Classification ---\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    config.MODEL_NAME,\n    num_labels=len(label_map)\n).to(config.DEVICE)\n\n# --- Dataloaders ---\ntrain_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE)\ntest_dataloader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE)\n\n# --- Optimizer and Scheduler ---\noptimizer = AdamW(model.parameters(), lr=config.LR)\ntotal_steps = len(train_dataloader) * config.EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n# --- Instantiate Loss Function with Class Weights ---\nloss_fn = nn.CrossEntropyLoss(weight=weights_tensor).to(config.DEVICE)\n\n# --- Training Loop ---\nprint(\"Starting end-to-end fine-tuning...\")\nfor epoch in range(config.EPOCHS):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{config.EPOCHS}\"):\n        input_ids = batch['input_ids'].to(config.DEVICE)\n        attention_mask = batch['attention_mask'].to(config.DEVICE)\n        labels = batch['labels'].to(config.DEVICE)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Calculate custom weighted loss\n        loss = loss_fn(outputs.logits, labels)\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch + 1} | Average Training Loss: {avg_train_loss:.4f}\")\n\n    # --- Validation after each epoch ---\n    print(\"Evaluating on the validation set...\")\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_dataloader, desc=\"Validating\"):\n            input_ids = batch['input_ids'].to(config.DEVICE)\n            attention_mask = batch['attention_mask'].to(config.DEVICE)\n            labels = batch['labels'].to(config.DEVICE)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # --- HIGHLIGHT ACCURACY ---\n    val_accuracy = accuracy_score(all_labels, all_preds)\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    target_names = [reverse_label_map[i] for i in range(len(label_map))]\n    print(classification_report(all_labels, all_preds, target_names=target_names))\n    print(\"=\"*50)\n    print(f\" Validation Accuracy for Epoch {epoch + 1}: {val_accuracy:.4f}\")\n    print(\"=\"*50)\n    print(\"-\" * 30)\n\nprint(\"Training complete.\")\nprint(\"-\" * 30)","metadata":{"id":"h9Wn0DObNvyV","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T04:11:12.370892Z","iopub.execute_input":"2025-07-19T04:11:12.371108Z","iopub.status.idle":"2025-07-19T05:03:04.470736Z","shell.execute_reply.started":"2025-07-19T04:11:12.371092Z","shell.execute_reply":"2025-07-19T05:03:04.470066Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdcf7f5ffef24f2bbf6f184c4f33149e"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting end-to-end fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/2:   0%|          | 0/3960 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10fc3423a01049768a128ec62aa1ad5a"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Average Training Loss: 0.5458\nEvaluating on the validation set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/990 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3138203f76a44bee89f30a8d161bd512"}},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    positive       0.97      0.94      0.96     12713\n    negative       0.70      0.83      0.76      1960\n     neutral       0.78      0.82      0.80      1167\n\n    accuracy                           0.92     15840\n   macro avg       0.82      0.86      0.84     15840\nweighted avg       0.93      0.92      0.92     15840\n\n==================================================\n Validation Accuracy for Epoch 1: 0.9179\n==================================================\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/2:   0%|          | 0/3960 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2221b7a67d0c4cd89ce6bcc46b419a57"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Average Training Loss: 0.4367\nEvaluating on the validation set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/990 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f578d3ffda4d538e94c7f9021667af"}},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    positive       0.97      0.96      0.96     12713\n    negative       0.75      0.81      0.78      1960\n     neutral       0.88      0.79      0.83      1167\n\n    accuracy                           0.93     15840\n   macro avg       0.87      0.86      0.86     15840\nweighted avg       0.93      0.93      0.93     15840\n\n==================================================\n Validation Accuracy for Epoch 2: 0.9317\n==================================================\n------------------------------\nTraining complete.\n------------------------------\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"## Final Evaluation on Test Set","metadata":{"id":"ZNyhhQsQEITG"}},{"cell_type":"code","source":"print(\"Performing final evaluation on the test set...\")\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n        input_ids = batch['input_ids'].to(config.DEVICE)\n        attention_mask = batch['attention_mask'].to(config.DEVICE)\n        labels = batch['labels'].to(config.DEVICE)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# --- Print Classification Report for Test Set ---\nprint(\"\\nTest Set Classification Report:\")\ntarget_names = [reverse_label_map[i] for i in range(len(label_map))]\nprint(classification_report(all_labels, all_preds, target_names=target_names))\n\ntest_accuracy = accuracy_score(all_labels, all_preds)\nprint(\"=\"*50)\nprint(f\"Final Test Set Accuracy: {test_accuracy:.4f}\")\nprint(\"=\"*50)\n","metadata":{"id":"ZNaTYU_aN37-","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T05:03:04.471443Z","iopub.execute_input":"2025-07-19T05:03:04.471659Z","iopub.status.idle":"2025-07-19T05:05:23.671463Z","shell.execute_reply.started":"2025-07-19T05:03:04.471643Z","shell.execute_reply":"2025-07-19T05:05:23.670897Z"}},"outputs":[{"name":"stdout","text":"Performing final evaluation on the test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1238 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6758e58ae7344b3aba02cb6a368781ee"}},"metadata":{}},{"name":"stdout","text":"\nTest Set Classification Report:\n              precision    recall  f1-score   support\n\n    positive       0.96      0.96      0.96     15891\n    negative       0.76      0.81      0.78      2450\n     neutral       0.88      0.78      0.82      1459\n\n    accuracy                           0.93     19800\n   macro avg       0.87      0.85      0.86     19800\nweighted avg       0.93      0.93      0.93     19800\n\n==================================================\nFinal Test Set Accuracy: 0.9305\n==================================================\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"7-9oGT0zENjc"}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*30)\nprint(\"Inference Section\")\nprint(\"=\"*30)\n\n# Create a reverse map to decode predictions\nreverse_label_map = {v: k for k, v in label_map.items()}\n\ndef predict_sentiment(texts, model, tokenizer, device, max_len=128):\n    \"\"\"\n    Predicts the sentiment for a list of texts.\n\n    Args:\n        texts (list of str): The new texts to analyze.\n        model: The fine-tuned model.\n        tokenizer: The tokenizer.\n        device: The device to run the model on (e.g., 'cuda' or 'cpu').\n        max_len (int): The maximum sequence length.\n\n    Returns:\n        list of str: The predicted sentiment labels.\n    \"\"\"\n    model.eval() # Set the model to evaluation mode\n\n    predictions = []\n\n    with torch.no_grad(): # No need to calculate gradients for inference\n        for text in texts:\n            encoding = tokenizer.encode_plus(\n                text,\n                add_special_tokens=True,\n                max_length=max_len,\n                return_token_type_ids=False,\n                padding='max_length',\n                truncation=True,\n                return_attention_mask=True,\n                return_tensors='pt',\n            )\n\n            input_ids = encoding['input_ids'].to(device)\n            attention_mask = encoding['attention_mask'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            # Get the prediction with the highest probability\n            pred_index = torch.argmax(logits, dim=1).item()\n            predicted_label = reverse_label_map[pred_index]\n            predictions.append(predicted_label)\n\n    return predictions\n\n# --- Example Usage ---\nnew_reviews = [\n    \"This app is absolutely fantastic! It works perfectly and has all the features I need.\",\n    \"I'm really disappointed with the latest update. It's buggy and crashes all the time.\",\n    \"The application is okay, it does what it says but nothing more.\",\n    \"I hate this, it's the worst experience I've ever had.\",\n    \"Wow, I love it! So useful and easy to use.\",\n    \"just it\"\n]\n\npredicted_sentiments = predict_sentiment(new_reviews, model, tokenizer, config.DEVICE, config.MAX_LENGTH)\n\nfor review, sentiment in zip(new_reviews, predicted_sentiments):\n    print(f\"Review: '{review}'\\nPredicted Sentiment: {sentiment}\\n\")\n","metadata":{"id":"w7zbxJzLS6r-","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T05:05:23.672324Z","iopub.execute_input":"2025-07-19T05:05:23.672538Z","iopub.status.idle":"2025-07-19T05:05:23.736852Z","shell.execute_reply.started":"2025-07-19T05:05:23.672522Z","shell.execute_reply":"2025-07-19T05:05:23.736222Z"}},"outputs":[{"name":"stdout","text":"\n==============================\nInference Section\n==============================\nReview: 'This app is absolutely fantastic! It works perfectly and has all the features I need.'\nPredicted Sentiment: positive\n\nReview: 'I'm really disappointed with the latest update. It's buggy and crashes all the time.'\nPredicted Sentiment: negative\n\nReview: 'The application is okay, it does what it says but nothing more.'\nPredicted Sentiment: positive\n\nReview: 'I hate this, it's the worst experience I've ever had.'\nPredicted Sentiment: negative\n\nReview: 'Wow, I love it! So useful and easy to use.'\nPredicted Sentiment: positive\n\nReview: 'just it'\nPredicted Sentiment: neutral\n\n","output_type":"stream"}],"execution_count":38}]}